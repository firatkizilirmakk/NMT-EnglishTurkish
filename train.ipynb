{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f235ed6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import string\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import fasttext\n",
    "import sklearn\n",
    "\n",
    "import models\n",
    "from trtokenizer.tr_tokenizer import SentenceTokenizer, WordTokenizer\n",
    "\n",
    "import unicodedata\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "super-confidentiality",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAFRAME_PATH = './dataframes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "trying-completion",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "news_df = pd.read_csv('dataframes/wmt-news.csv')\n",
    "news_df = news_df.drop('partition', axis = 1)\n",
    "news_df.en = news_df.en.apply(lambda x: str(x).replace('\\n', ''))\n",
    "news_df.tr = news_df.tr.apply(lambda x: str(x).replace('\\n', ''))\n",
    "news_df\n",
    "\n",
    "train_val_df = news_df.sample(9000, random_state = 7)\n",
    "test_df = news_df[~news_df.index.isin(train_val_df.index)]\n",
    "\n",
    "train_df = train_val_df.sample(8500, random_state = 7)\n",
    "valid_df = train_val_df[~train_val_df.index.isin(train_df.index)]\n",
    "\n",
    "len(train_df), len(valid_df), len(test_df)\n",
    "\n",
    "iwslt_df = pd.read_csv('dataframes/iwslt14.csv')\n",
    "\n",
    "iwslt_train_val_df = iwslt_df.sample(2000, random_state = 7)\n",
    "iwslt_test_df = iwslt_df[~iwslt_df.index.isin(iwslt_train_val_df.index)]\n",
    "\n",
    "iwslt_train_df = iwslt_train_val_df.sample(1500, random_state = 7)\n",
    "iwslt_valid_df = iwslt_train_val_df[~iwslt_train_val_df.index.isin(iwslt_train_df.index)]\n",
    "\n",
    "len(iwslt_train_df), len(iwslt_valid_df), len(iwslt_test_df)\n",
    "\n",
    "train_df['split'] = 'train'\n",
    "valid_df['split'] = 'validation'\n",
    "test_df['split']  = 'test'\n",
    "\n",
    "iwslt_train_df['split'] = 'train'\n",
    "iwslt_valid_df['split'] = 'validation'\n",
    "iwslt_test_df['split'] = 'test'\n",
    "\n",
    "df = pd.concat([train_df, valid_df, test_df, iwslt_train_df, iwslt_valid_df, iwslt_test_df])\n",
    "df = df.reset_index(drop= True)\n",
    "df.to_csv('dataframes/combined-iwslt-news.csv')\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "speaking-toilet",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "df = pd.read_csv(os.path.join('dataframes', 'wmt16.csv'))\n",
    "df.split.unique()\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "with open('en-fr-data/eng-fra.txt') as f:\n",
    "    lines = f.read().strip().split('\\n')\n",
    "    lines = [line.split('\\t') for line in lines]\n",
    "\n",
    "train, test = sklearn.model_selection.train_test_split(lines, test_size = 0.1, train_size = 0.9)\n",
    "train, valid = sklearn.model_selection.train_test_split(train, test_size = 0.1, train_size = 0.9)\n",
    "\n",
    "en_samples = []\n",
    "fr_samples = []\n",
    "\n",
    "for sample in test:\n",
    "    en, fr = sample\n",
    "\n",
    "    en = normalizeString(en)\n",
    "    fr = normalizeString(fr)\n",
    "\n",
    "    en_samples.append(en)\n",
    "    fr_samples.append(fr)\n",
    "\n",
    "def create_df(en_samples, fr_samples, split:str):\n",
    "    train_df = pd.DataFrame({'en': [], 'fr': [], 'split': []})\n",
    "\n",
    "    train_df.en = en_samples\n",
    "    train_df.fr = fr_samples\n",
    "    train_df.split = split\n",
    "    return train_df\n",
    "\n",
    "train_df = create_df(en_samples, fr_samples, 'train')\n",
    "\n",
    "valid_df = create_df(en_samples, fr_samples, 'validation')\n",
    "\n",
    "test_df = create_df(en_samples, fr_samples, 'test')\n",
    "\n",
    "pd.concat([train_df, valid_df, test_df]).reset_index(drop = True).to_csv('en_fr.csv')\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5afd8296",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16d80c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, loss, epoch, path):\n",
    "    checkpoint = {\n",
    "        \"epoch\" : epoch,\n",
    "        \"loss\" : loss,\n",
    "        \"model_state_dict\" : model.state_dict(),\n",
    "        \"optimizer_state_dict\" : optimizer.state_dict(),\n",
    "    }\n",
    "\n",
    "    torch.save(checkpoint, path)\n",
    "\n",
    "def load_checkpoint(model, path):\n",
    "    checkpoint = torch.load(path, map_location = device)\n",
    "\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model = model.to(device)\n",
    "\n",
    "    epoch = checkpoint[\"epoch\"]\n",
    "    loss = checkpoint[\"loss\"]\n",
    "\n",
    "    return epoch, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24fc819",
   "metadata": {},
   "source": [
    "## Train Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91ae5090",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCH  = 30\n",
    "BATCH_SIZE = 12\n",
    "INITIAL_LR = 0.001\n",
    "MODEL_TYPE = 'recurrent'\n",
    "\n",
    "CLIP = 5 # ??\n",
    "TRAIN = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7e681c",
   "metadata": {},
   "source": [
    "## Data Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49d5d4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAFRAME_PATH = './dataframes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c09d8173",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_word_tokenizer = WordTokenizer()\n",
    "\n",
    "def en_tokenizer(text: str) -> list:\n",
    "    return nltk.word_tokenize(text, language = 'english')\n",
    "\n",
    "def tr_tokenizer(text: str) -> list:\n",
    "    return tr_word_tokenizer.tokenize(text)\n",
    "\n",
    "def fr_tokenizer(text: str) -> list:\n",
    "    return nltk.word_tokenize(text, language = 'french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f8ef958",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(DATAFRAME_PATH, 'combined-iwslt-news.csv'))\n",
    "\n",
    "train_df = df[df.split == 'train']\n",
    "valid_df = df[df.split == 'validation']\n",
    "\n",
    "valid_df = valid_df.reset_index(drop = True)\n",
    "train_df = train_df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "published-bubble",
   "metadata": {},
   "outputs": [],
   "source": [
    "wmt_df = pd.read_csv(os.path.join(DATAFRAME_PATH, 'wmt16.csv'))\n",
    "wmt_train = wmt_df[wmt_df.split == 'train']\n",
    "wmt_train = wmt_train.sample(5000, random_state = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "confirmed-residence",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df = pd.concat([train_df, wmt_train]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dc5a24",
   "metadata": {},
   "source": [
    "### Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "standing-sydney",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTDataset(Dataset):\n",
    "    def __init__(self, en_series, tr_series, en_vocab, tr_vocab):\n",
    "        self.en_series = en_series\n",
    "        self.tr_series = tr_series\n",
    "        self.en_vocab = en_vocab\n",
    "        self.tr_vocab = tr_vocab\n",
    "\n",
    "        assert len(en_series) == len(tr_series)\n",
    "        self.ds_len = len(self.en_series)\n",
    "        self.data = []\n",
    "\n",
    "        self.convert_text_to_tokens()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.ds_len\n",
    "\n",
    "    def __getitem__(self, indx):\n",
    "        sample = self.data[indx]\n",
    "        return {'src': sample[0], 'trg': sample[1]}\n",
    "\n",
    "    def convert_text_to_tokens(self):\n",
    "        for i in range(len(self.en_series)):\n",
    "            en_tensor = torch.tensor([self.en_vocab[token] for token in self.en_series[i]], dtype = torch.long)\n",
    "            tr_tensor = torch.tensor([self.tr_vocab[token] for token in self.tr_series[i]], dtype = torch.long)\n",
    "            self.data.append((en_tensor, tr_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "signal-irish",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tr_fasttext = fasttext.load_model('cc.tr.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3347407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_preprocess(x, field):\n",
    "    def apply_token_replacement(token):\n",
    "        return token.replace('.', '').replace(',', '').replace('“', '')\n",
    "\n",
    "    out = field.preprocess(str(x))\n",
    "    return [str(token.translate(str.maketrans('', '', string.punctuation))) for token in out]\n",
    "    #return [apply_token_replacement(token) for token in out]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f13df77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpora_dataset(en_text: str, tr_text: str, en_vocab, tr_vocab):\n",
    "    data = []\n",
    "\n",
    "    for i in range(len(en_text)):\n",
    "        en_tensor = torch.tensor([en_vocab[token] for token in en_text[i]], dtype = torch.long)\n",
    "        tr_tensor = torch.tensor([tr_vocab[token] for token in tr_text[i]], dtype = torch.long)\n",
    "        data.append((en_tensor, tr_tensor))\n",
    "\n",
    "    return data\n",
    "\n",
    "def generate_batch(data_batch):\n",
    "    en_batch, tr_batch = [], []\n",
    "\n",
    "    for (en_item, tr_item) in data_batch:\n",
    "        en_batch.append(torch.cat([torch.tensor([SOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "        tr_batch.append(torch.cat([torch.tensor([SOS_IDX]), tr_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "\n",
    "    en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
    "    tr_batch = pad_sequence(tr_batch, padding_value=PAD_IDX)\n",
    "\n",
    "    return en_batch, tr_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "wired-carolina",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_batch(data_batch):\n",
    "    en_batch, tr_batch = [], []\n",
    "\n",
    "    for item in data_batch:\n",
    "        en_item = item['src']\n",
    "        tr_item = item['trg']\n",
    "\n",
    "        en_batch.append(torch.cat([torch.tensor([SOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "        tr_batch.append(torch.cat([torch.tensor([SOS_IDX]), tr_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "\n",
    "    en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
    "    tr_batch = pad_sequence(tr_batch, padding_value=PAD_IDX)\n",
    "\n",
    "    return en_batch, tr_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0591a9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_field = Field(tokenize = en_tokenizer, init_token='<sos>', eos_token='<eos>')\n",
    "tr_field  = Field(tokenize = tr_tokenizer, init_token='<sos>', eos_token='<eos>')\n",
    "\n",
    "# get preprocessed train data\n",
    "en_train_preprocessed_text = train_df['en'].apply(lambda x: apply_preprocess(x, en_field))\n",
    "tr_train_preprocessed_text = train_df['tr'].apply(lambda x: apply_preprocess(x, tr_field))\n",
    "\n",
    "# get preprocessed train data\n",
    "en_valid_preprocessed_text = valid_df['en'].apply(lambda x: apply_preprocess(x, en_field))\n",
    "tr_valid_preprocessed_text = valid_df['tr'].apply(lambda x: apply_preprocess(x, tr_field))\n",
    "\n",
    "# build vocabulary for the languages\n",
    "en_field.build_vocab(pd.concat([en_train_preprocessed_text, en_valid_preprocessed_text]), min_freq = 2)\n",
    "tr_field.build_vocab(pd.concat([tr_train_preprocessed_text, tr_valid_preprocessed_text]), min_freq = 2)\n",
    "\n",
    "en_vocab = en_field.vocab\n",
    "tr_vocab = tr_field.vocab\n",
    "\n",
    "# define tags\n",
    "# only taking values from tr since they are equal in en_vocab as well\n",
    "PAD_IDX = tr_vocab['<pad>']\n",
    "SOS_IDX = tr_vocab['<sos>']\n",
    "EOS_IDX = tr_vocab['<eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "studied-heather",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    train_dataset = NMTDataset(en_train_preprocessed_text, tr_train_preprocessed_text, en_vocab, tr_vocab)\n",
    "    valid_dataset = NMTDataset(en_valid_preprocessed_text, tr_valid_preprocessed_text, en_vocab, tr_vocab)\n",
    "\n",
    "    train_loader = BucketIterator(train_dataset, batch_size = BATCH_SIZE, \n",
    "                                                        sort_key = (lambda x: (len(x['src']) + len(x['trg']))), \n",
    "                                                        repeat=True,  sort=False,  shuffle=True, \n",
    "                                                        sort_within_batch=True)\n",
    "\n",
    "    valid_loader = BucketIterator(valid_dataset, batch_size = BATCH_SIZE, \n",
    "                                                        sort_key = (lambda x: (len(x['src']) + len(x['trg']))), \n",
    "                                                        repeat=True,  sort=False,  shuffle=False, \n",
    "                                                        sort_within_batch=True)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #train_dataset = get_corpora_dataset(en_train_preprocessed_text, tr_train_preprocessed_text, en_vocab, tr_vocab)\n",
    "    #valid_dataset = get_corpora_dataset(en_valid_preprocessed_text, tr_valid_preprocessed_text, en_vocab, tr_vocab)\n",
    "\n",
    "    #train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn = generate_batch)\n",
    "    #valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn = generate_batch)\n",
    "\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "precious-douglas",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9195, 7067)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tr_vocab.itos[4:]), len(en_vocab.itos[4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "honest-creativity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rebull and her colleagues detailed their latest analysis of Pleiades spin rates in three new papers  soon to be published in the Astronomical Journal '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(en_train_preprocessed_text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "electrical-ceiling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rebull ve meslektaşları Ülker takım yıldızının dönüş hızları ile ilgili en yeni analizlerini kısa süre içinde Astronomi Dergisi nde yayınlanacak üç yeni makalede ayrıntılı olarak açıklamıştır '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(tr_train_preprocessed_text[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805a3610",
   "metadata": {},
   "source": [
    "## Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "frequent-rental",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "    if type(m) == nn.LSTM:\n",
    "        for param in m._flat_weights_names:\n",
    "            if \"weight\" in param:\n",
    "                nn.init.xavier_uniform_(m._parameters[param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b35a014",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(en_vocab)\n",
    "OUTPUT_DIM = len(tr_vocab)\n",
    "\n",
    "# embedding hyperparams\n",
    "ENC_EMB_DIM = 512\n",
    "DEC_EMB_DIM = 512\n",
    "\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "# model hyperparams\n",
    "if MODEL_TYPE == 'attention':\n",
    "    ENC_HID_DIM = 512\n",
    "    DEC_HID_DIM = 512\n",
    "\n",
    "    attn = models.Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "    enc  = models.AttentionEncoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "    dec  = models.AttentionDecoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "    model = models.AttentionSeq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "elif MODEL_TYPE == 'recurrent':\n",
    "    HID_DIM = 1024\n",
    "    N_LAYERS = 2\n",
    "\n",
    "    enc = models.Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "    dec = models.Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "    model = models.Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "    model.apply(xavier_init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9cc6cb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "TRG_PAD_IDX = tr_vocab.stoi[tr_field.pad_token]\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=INITIAL_LR)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2)  # goal: maximize Dice score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee62c1c",
   "metadata": {},
   "source": [
    "### Train & Eval Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "least-covering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU SCORE\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "def calculate_bleu(gt_trg, pred_trg):\n",
    "    bleu_scores = 0\n",
    "    batch_size = gt_trg.shape[1]\n",
    "    for i in range(batch_size):\n",
    "        gt_sentence   = gt_trg[:, i]\n",
    "        pred_sentence = pred_trg[:, i, :]\n",
    "\n",
    "        gt_sentence   = [tr_vocab.itos[token_id] for token_id in gt_sentence]\n",
    "        # greedy decoding\n",
    "        pred_sentence = [tr_vocab.itos[torch.argmax(output_distr)] for output_distr in pred_sentence]\n",
    "\n",
    "        bleu_score = nltk.translate.bleu_score.sentence_bleu([gt_sentence], pred_sentence, weights = [0.5, 0.5])\n",
    "        bleu_scores += bleu_score\n",
    "\n",
    "    return bleu_scores / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac31f0a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [001]:   0%|          | 2/834 [00:00<00:52, 15.71it/s, bleu=0.064, loss=8.98, ppl=7.93e+3]/home/pc3/workspace/venv/lib/python3.8/site-packages/nltk/translate/bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, TrainLoss: 5.68, ValidLoss : 5.24, TrainBleu:               0.094153, ValidBleu: 0.156235, lr: 0.001\n",
      "Saving model_checkpoints/recurrent_model.pkl at EPOCH: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, TrainLoss: 3.53, ValidLoss : 4.23, TrainBleu:               0.271857, ValidBleu: 0.281767, lr: 0.001\n",
      "Saving model_checkpoints/recurrent_model.pkl at EPOCH: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, TrainLoss: 2.36, ValidLoss : 3.93, TrainBleu:               0.448526, ValidBleu: 0.327493, lr: 0.001\n",
      "Saving model_checkpoints/recurrent_model.pkl at EPOCH: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, TrainLoss: 1.86, ValidLoss : 3.81, TrainBleu:               0.522694, ValidBleu: 0.359017, lr: 0.0001\n",
      "Saving model_checkpoints/recurrent_model.pkl at EPOCH: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, TrainLoss: 1.51, ValidLoss : 3.74, TrainBleu:               0.583258, ValidBleu: 0.384174, lr: 0.0001\n",
      "Saving model_checkpoints/recurrent_model.pkl at EPOCH: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, TrainLoss: 1.43, ValidLoss : 3.72, TrainBleu:               0.600685, ValidBleu: 0.387930, lr: 0.0001\n",
      "Saving model_checkpoints/recurrent_model.pkl at EPOCH: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, TrainLoss: 1.36, ValidLoss : 3.71, TrainBleu:               0.610438, ValidBleu: 0.395118, lr: 1e-05\n",
      "Saving model_checkpoints/recurrent_model.pkl at EPOCH: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, TrainLoss: 1.31, ValidLoss : 3.71, TrainBleu:               0.618038, ValidBleu: 0.396841, lr: 1e-05\n",
      "Saving model_checkpoints/recurrent_model.pkl at EPOCH: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, TrainLoss: 1.29, ValidLoss : 3.71, TrainBleu:               0.618863, ValidBleu: 0.396929, lr: 1e-05\n",
      "Saving model_checkpoints/recurrent_model.pkl at EPOCH: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, TrainLoss: 1.29, ValidLoss : 3.71, TrainBleu:               0.619489, ValidBleu: 0.396490, lr: 1.0000000000000002e-06\n",
      "Saving model_checkpoints/recurrent_model.pkl at EPOCH: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, TrainLoss: 1.28, ValidLoss : 3.71, TrainBleu:               0.621212, ValidBleu: 0.396663, lr: 1.0000000000000002e-06\n",
      "Saving model_checkpoints/recurrent_model.pkl at EPOCH: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, TrainLoss: 1.28, ValidLoss : 3.71, TrainBleu:               0.620130, ValidBleu: 0.396861, lr: 1.0000000000000002e-06\n",
      "Saving model_checkpoints/recurrent_model.pkl at EPOCH: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, TrainLoss: 1.28, ValidLoss : 3.71, TrainBleu:               0.621564, ValidBleu: 0.397497, lr: 1.0000000000000002e-07\n",
      "Saving model_checkpoints/recurrent_model.pkl at EPOCH: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, TrainLoss: 1.28, ValidLoss : 3.71, TrainBleu:               0.621800, ValidBleu: 0.397383, lr: 1.0000000000000002e-07\n",
      "Saving model_checkpoints/recurrent_model.pkl at EPOCH: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, TrainLoss: 1.28, ValidLoss : 3.71, TrainBleu:               0.620399, ValidBleu: 0.397383, lr: 1.0000000000000002e-07\n",
      "Saving model_checkpoints/recurrent_model.pkl at EPOCH: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, TrainLoss: 1.28, ValidLoss : 3.71, TrainBleu:               0.620988, ValidBleu: 0.397384, lr: 1.0000000000000004e-08\n",
      "Saving model_checkpoints/recurrent_model.pkl at EPOCH: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, TrainLoss: 1.28, ValidLoss : 3.71, TrainBleu:               0.621196, ValidBleu: 0.397384, lr: 1.0000000000000004e-08\n",
      "Saving model_checkpoints/recurrent_model.pkl at EPOCH: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, TrainLoss: 1.28, ValidLoss : 3.71, TrainBleu:               0.619934, ValidBleu: 0.397384, lr: 1.0000000000000004e-08\n",
      "Saving model_checkpoints/recurrent_model.pkl at EPOCH: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, TrainLoss: 1.28, ValidLoss : 3.71, TrainBleu:               0.620584, ValidBleu: 0.397384, lr: 1.0000000000000004e-08\n",
      "Saving model_checkpoints/recurrent_model.pkl at EPOCH: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, TrainLoss: 1.28, ValidLoss : 3.71, TrainBleu:               0.621233, ValidBleu: 0.397384, lr: 1.0000000000000004e-08\n",
      "Saving model_checkpoints/recurrent_model.pkl at EPOCH: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, TrainLoss: 1.28, ValidLoss : 3.71, TrainBleu:               0.620080, ValidBleu: 0.397384, lr: 1.0000000000000004e-08\n",
      "Saving model_checkpoints/recurrent_model.pkl at EPOCH: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22, TrainLoss: 1.28, ValidLoss : 3.71, TrainBleu:               0.619876, ValidBleu: 0.397384, lr: 1.0000000000000004e-08\n",
      "Saving model_checkpoints/recurrent_model.pkl at EPOCH: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23, TrainLoss: 1.28, ValidLoss : 3.71, TrainBleu:               0.620879, ValidBleu: 0.397384, lr: 1.0000000000000004e-08\n",
      "Saving model_checkpoints/recurrent_model.pkl at EPOCH: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24, TrainLoss: 1.28, ValidLoss : 3.71, TrainBleu:               0.621777, ValidBleu: 0.397384, lr: 1.0000000000000004e-08\n",
      "Saving model_checkpoints/recurrent_model.pkl at EPOCH: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25, TrainLoss: 1.28, ValidLoss : 3.71, TrainBleu:               0.620689, ValidBleu: 0.397384, lr: 1.0000000000000004e-08\n",
      "Saving model_checkpoints/recurrent_model.pkl at EPOCH: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26, TrainLoss: 1.28, ValidLoss : 3.71, TrainBleu:               0.619976, ValidBleu: 0.397384, lr: 1.0000000000000004e-08\n",
      "Saving model_checkpoints/recurrent_model.pkl at EPOCH: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27, TrainLoss: 1.28, ValidLoss : 3.71, TrainBleu:               0.621437, ValidBleu: 0.397384, lr: 1.0000000000000004e-08\n",
      "Saving model_checkpoints/recurrent_model.pkl at EPOCH: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28, TrainLoss: 1.28, ValidLoss : 3.71, TrainBleu:               0.620296, ValidBleu: 0.397384, lr: 1.0000000000000004e-08\n",
      "Saving model_checkpoints/recurrent_model.pkl at EPOCH: 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29, TrainLoss: 1.28, ValidLoss : 3.71, TrainBleu:               0.621645, ValidBleu: 0.397384, lr: 1.0000000000000004e-08\n",
      "Saving model_checkpoints/recurrent_model.pkl at EPOCH: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30, TrainLoss: 1.28, ValidLoss : 3.71, TrainBleu:               0.621256, ValidBleu: 0.397384, lr: 1.0000000000000004e-08\n",
      "Saving model_checkpoints/recurrent_model.pkl at EPOCH: 30\n"
     ]
    }
   ],
   "source": [
    "if TRAIN:\n",
    "    best_valid_loss = 999999\n",
    "    plateau_counter = 0\n",
    "\n",
    "    for epoch in range(MAX_EPOCH):\n",
    "        train_loader.create_batches()\n",
    "        valid_loader.create_batches()\n",
    "\n",
    "        train_looper = tqdm(enumerate(train_loader.batches), total=len(train_loader), leave = False, position = 0)\n",
    "        train_looper.set_description(\"Epoch [{:003}]\".format(epoch + 1))\n",
    "\n",
    "        epoch_train_loss = 0\n",
    "        epoch_valid_loss = 0\n",
    "        train_bleu_score = 0\n",
    "        valid_bleu_score = 0\n",
    "\n",
    "        #train\n",
    "        model.train()\n",
    "        for i, batch in train_looper:\n",
    "            src, trg = pad_batch(batch)\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(src, trg)\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            bleu_score = calculate_bleu(trg, output)\n",
    "            train_bleu_score += bleu_score\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = loss_fn(output, trg)\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "\n",
    "            optimizer.step()\n",
    "            epoch_train_loss += loss.item()\n",
    "            train_looper.set_postfix(loss=loss.detach().item(), bleu = bleu_score, ppl= math.exp(loss))\n",
    "\n",
    "        #evaluate\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for _, (batch) in enumerate(valid_loader.batches):\n",
    "                src, trg = pad_batch(batch)\n",
    "                src = src.to(device)\n",
    "                trg = trg.to(device)\n",
    "\n",
    "                output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "                bleu_score = calculate_bleu(trg, output)\n",
    "                valid_bleu_score += bleu_score\n",
    "                \n",
    "                output = output[1:].view(-1, output.shape[-1])\n",
    "                trg = trg[1:].view(-1)\n",
    "\n",
    "                loss = loss_fn(output, trg)\n",
    "                epoch_valid_loss += loss.item()\n",
    "\n",
    "\n",
    "        epoch_train_loss = epoch_train_loss / len(train_loader)\n",
    "        epoch_valid_loss = epoch_valid_loss / len(valid_loader)\n",
    "        train_bleu_score = train_bleu_score / len(train_loader)\n",
    "        valid_bleu_score = valid_bleu_score / len(valid_loader)\n",
    "\n",
    "        scheduler.step(epoch_valid_loss)\n",
    "\n",
    "        print(\"Epoch: {}, TrainLoss: {:.2f}, ValidLoss : {:.2f}, TrainBleu: \\\n",
    "              {:2f}, ValidBleu: {:2f}, lr: {}\".format(epoch + 1, epoch_train_loss, \n",
    "                                                      epoch_valid_loss, train_bleu_score, valid_bleu_score,\n",
    "                                                      optimizer.param_groups[0]['lr']))    \n",
    "\n",
    "        # checkpoint\n",
    "        #if epoch_valid_loss < best_valid_loss:\n",
    "        if True:\n",
    "            plateau_counter = 0\n",
    "            best_valid_loss = epoch_valid_loss\n",
    "            checkpoint_path = 'model_checkpoints/' + str(MODEL_TYPE) + \"_model.pkl\"\n",
    "            print(\"Saving {} at EPOCH: {}\".format(checkpoint_path, epoch + 1))\n",
    "            save_checkpoint(model, optimizer, epoch_valid_loss, (epoch + 1), checkpoint_path)\n",
    "        else:\n",
    "            plateau_counter += 1\n",
    "            if plateau_counter > 5:\n",
    "                print(\"Early stopping...\")\n",
    "                #break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-embassy",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "behavioral-feeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(DATAFRAME_PATH, 'combined-iwslt-news.csv'))\n",
    "\n",
    "test_df = df[df.split == 'test']\n",
    "test_df = test_df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "essential-generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_test_preprocessed_text = test_df['en'].apply(lambda x: en_field.preprocess(x))\n",
    "tr_test_preprocessed_text = test_df['tr'].apply(lambda x: tr_field.preprocess(x))\n",
    "\n",
    "test_dataset = get_corpora_dataset(en_test_preprocessed_text, tr_test_preprocessed_text, en_vocab, tr_vocab)\n",
    "test_loader = DataLoader(test_dataset, batch_size=12, shuffle=False, collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "referenced-flush",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 3.710268461397694)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_checkpoint(model, 'model_checkpoints/' + str(MODEL_TYPE) + '_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "focal-coach",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU SCORE\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "def get_bleu_batch(gt_trg, pred_trg):\n",
    "    batch_size = gt_trg.shape[1]\n",
    "\n",
    "    gt_sentences = []\n",
    "    pred_sentences = []\n",
    "    bleu_score_list = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        gt_sentence   = gt_trg[:, i]\n",
    "        pred_sentence = pred_trg[:, i, :]\n",
    "\n",
    "        gt_sentence   = [tr_vocab.itos[token_id] for token_id in gt_sentence]\n",
    "        # greedy decoding\n",
    "        pred_sentence = [tr_vocab.itos[torch.argmax(output_distr)] for output_distr in pred_sentence]\n",
    "\n",
    "        bleu_score = nltk.translate.bleu_score.sentence_bleu([gt_sentence], pred_sentence, weights = [0.5, 0.5])\n",
    "\n",
    "        gt_sentences.append(gt_sentence)\n",
    "        pred_sentences.append(pred_sentence)\n",
    "        bleu_score_list.append(bleu_score)\n",
    "\n",
    "    return gt_sentences, pred_sentences, bleu_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "sufficient-occupation",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = 0\n",
    "#evaluate\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    bleu_scores = []\n",
    "    gt_sentences_list = []\n",
    "    pred_sentences_list = []\n",
    "\n",
    "    for _, (src, trg) in enumerate(test_loader):\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "\n",
    "        output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "        gt_sentences, pred_sentences, bleu_score_list = get_bleu_batch(trg, output)\n",
    "        bleu_scores.extend(bleu_score_list)\n",
    "        gt_sentences_list.extend(gt_sentences)\n",
    "        pred_sentences_list.extend(pred_sentences)\n",
    "\n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "        loss = loss_fn(output, trg)\n",
    "        test_loss += loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "outstanding-tablet",
   "metadata": {},
   "outputs": [],
   "source": [
    "indx = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "transsexual-senator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos> <unk> aylığı bağlama şartı olan 10 yıl <unk> 1800 günü olmayan hiç kimse emekli olamaz mı <unk> <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(gt_sentences_list[indx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "compound-contrary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk> <unk>   <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>   '"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(pred_sentences_list[indx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "separated-talent",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = test_loss / len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "matched-principal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 5.286, Test PPL: 197.480, Test Bleu: 0.121\n"
     ]
    }
   ],
   "source": [
    "print('Test Loss: {:.3f}, Test PPL: {:7.3f}, Test Bleu: {:.3f}'.format(test_loss, math.exp(test_loss), np.mean(bleu_scores)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
